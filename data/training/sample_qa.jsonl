{"instruction": "What is Retrieval-Augmented Generation (RAG)?", "input": "", "output": "Retrieval-Augmented Generation (RAG) is an AI framework that enhances large language model responses by grounding them in external, up-to-date, or domain-specific knowledge. Instead of relying solely on information baked into the model's weights during training, RAG retrieves relevant documents at query time and injects them into the context window before generation."}
{"instruction": "What are the three main stages of the RAG pipeline?", "input": "", "output": "The three main stages of the RAG pipeline are: 1) Ingestion - loading source documents, splitting into chunks, embedding them, and storing in a vector database; 2) Retrieval - embedding the user query and performing similarity search to find the most relevant chunks; 3) Generation - passing the retrieved chunks as context along with the question to the LLM to produce a grounded answer."}
{"instruction": "What is FAISS and why is it used in RAG systems?", "input": "", "output": "FAISS (Facebook AI Similarity Search) is an open-source library for efficient similarity search over dense vectors. In RAG systems it is used as the vector store because it is extremely fast at approximate nearest neighbor search, runs entirely in memory or on disk without a separate server, and supports millions of vectors. It is best suited for single-node, high-performance use cases."}
{"instruction": "How does Chroma differ from FAISS as a vector store?", "input": "", "output": "While FAISS is primarily an in-memory index optimized for raw speed, Chroma is a full embedding database with built-in persistence, metadata filtering, and a REST API. Chroma stores data as a proper database that survives restarts, supports querying by both semantic similarity and structured metadata filters, and is easier to self-host for production use cases."}
{"instruction": "Why do we split documents into overlapping chunks before embedding?", "input": "", "output": "Documents are split into overlapping chunks so that relevant passages are always fully captured in at least one chunk. Without overlap, a sentence that spans two chunk boundaries would be split in half, losing context. With overlap (e.g., 64 characters), consecutive chunks share a border region, ensuring no important information is cut off at boundaries. The chunk size also matters because embedding models have token limits and smaller chunks tend to produce more precise retrieval results."}
{"instruction": "What is LangChain Expression Language (LCEL)?", "input": "", "output": "LangChain Expression Language (LCEL) is a declarative, pipe-based syntax for composing AI chains. Instead of writing imperative code, you declare a pipeline using the pipe operator (|), for example: retriever | format_docs | prompt | llm | output_parser. Each component is a Runnable that implements .invoke(), .stream(), and .batch(), making components interchangeable and the overall chain composable and easy to reason about."}
{"instruction": "What embedding model is used by default and why?", "input": "", "output": "The default embedding model is all-MiniLM-L6-v2 from HuggingFace. It is chosen because it is small (~90MB), fast on CPU, requires no API key since it runs entirely locally, and produces strong 384-dimensional embeddings that work well for semantic search across most domains. It strikes a good balance between speed, size, and retrieval quality."}
{"instruction": "What is the difference between keyword search and semantic search?", "input": "", "output": "Keyword search matches exact words or phrases in documents, so it fails when the user uses different wording than the document. Semantic search uses embedding models to convert both the query and documents into dense vectors in a shared meaning space, so it can find relevant results even when no words overlap. For example, 'automobile' and 'car' would match semantically but not by keyword."}
{"instruction": "What is LoRA and why is it used for fine-tuning LLMs?", "input": "", "output": "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that freezes the original model weights and injects small trainable low-rank matrices into the attention layers. Instead of updating all billions of parameters, LoRA trains only a tiny fraction (often <1%), drastically reducing GPU memory requirements and training time. The LoRA adapters can be saved separately and merged back into the base model, making it easy to share and deploy fine-tuned models."}
{"instruction": "What is QLoRA?", "input": "", "output": "QLoRA (Quantized LoRA) combines LoRA with 4-bit quantization of the base model. The base model is loaded in 4-bit NormalFloat (NF4) precision using bitsandbytes, which reduces its memory footprint by ~4x compared to full precision. LoRA adapters are still trained in full precision. This allows fine-tuning models like LLaMA 3 (8B) on a single consumer GPU with 12-16GB VRAM that would otherwise require 40GB+."}
{"instruction": "What is an SFTTrainer?", "input": "", "output": "SFTTrainer (Supervised Fine-Tuning Trainer) is a class from HuggingFace's TRL library that simplifies the process of instruction-tuning language models. It handles tokenization of the dataset, masking of prompt tokens so the model only learns to generate the response, packing of examples for efficiency, and all the standard training loop boilerplate. It integrates with PEFT for LoRA training and supports streaming datasets."}
{"instruction": "How do you format data for instruction fine-tuning?", "input": "", "output": "For instruction fine-tuning, data is typically formatted in the Alpaca format: {\"instruction\": \"the question or task\", \"input\": \"optional context\", \"output\": \"the expected response\"}. During training this is converted into a chat template like: ### Instruction: {instruction}\\n### Input: {input}\\n### Response: {output}. The model learns to generate the response given the instruction, effectively learning your desired behavior on your specific task."}
