# ── RAG System ─────────────────────────────────────────────────────────────────
LLM_PROVIDER=ollama
LLM_MODEL=llama3.2
OPENAI_API_KEY=sk-your-key-here

EMBEDDING_MODEL=all-MiniLM-L6-v2
VECTOR_STORE=faiss
VECTOR_STORE_PATH=./vectorstore

CHUNK_SIZE=512
CHUNK_OVERLAP=64
TOP_K_RESULTS=4

FLASK_PORT=5050
FLASK_DEBUG=false

# ── Fine-Tuning ──────────────────────────────────────────────────────────────
# Base model to fine-tune (choose one):
#   TinyLlama/TinyLlama-1.1B-Chat-v1.0   (small, fast, no token needed)
#   microsoft/Phi-3-mini-4k-instruct      (recommended, no token needed)
#   meta-llama/Llama-3.2-1B-Instruct     (needs HuggingFace token)
FINETUNE_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0

# HuggingFace token (only needed for gated models like Llama)
HUGGINGFACE_TOKEN=hf_your_token_here

# Where to save the LoRA adapter after training
LORA_OUTPUT_DIR=./models/lora-adapter
